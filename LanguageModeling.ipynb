{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLmsjZCW51fI5/tloonTbK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasoriya/CS6120-PS2/blob/master/LanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWB9udaKesP",
        "colab_type": "text"
      },
      "source": [
        "Your task is to train *character-level* language models. \n",
        "You will train uni/bi/tri - gram character level models on the Gutenberg dataset. We will then use these trained English language models to segregate English documents from Brazilian Portuguese documents in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHFJmuftHJld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import httpimport\n",
        "\n",
        "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from lm_helper import get_train_data, get_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0pfuiEChTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the train and test data\n",
        "train = get_train_data()\n",
        "test, test_files = get_test_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WAO9VjFLArq",
        "colab_type": "text"
      },
      "source": [
        "## 1.1\n",
        "Report the unigram, bigram, and trigram character counts. You can submit them as separate files in your repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4VOoiSIoUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3mnaIvQnhI",
        "colab_type": "text"
      },
      "source": [
        "## 1.2\n",
        "Calculate the perplexity for each document in the test set using linear interpolation smoothing method. For determining λs for linear interpolation, you can divide the training data into a new training set (80%) and a held-out set (20%), then using grid search method:\n",
        "- First, report the λs chosen from the grid search, and explain why it's chosen.\n",
        "- Some documents in the test set are in Brazilian-Portuguese. Identify them as follows: \n",
        "    - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian-Portuguese. \n",
        "    - Print the file names and scores of the documents above the threshold\n",
        "\n",
        "    ```\n",
        "        file name, score\n",
        "        file name, score\n",
        "        . . .\n",
        "        file name, score\n",
        "    ```\n",
        "\n",
        "    - Manually check them and report if they are Brazilian-Portuguese or not.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQF4HhQGOZD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQl2u_giVW5e",
        "colab_type": "text"
      },
      "source": [
        "## 1.3\n",
        "Build another language model with add-λ smoothing (use λ = 0.1).\n",
        "- Then identify Brazilian-Portuguese documents as follows: \n",
        "  - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian-Portuguese. \n",
        "  - Print the file names and scores of the documents above the threshold\n",
        "\n",
        "  ```\n",
        "      file name, score\n",
        "      file name, score\n",
        "      . . .\n",
        "      file name, score\n",
        "  ```\n",
        "\n",
        "  - Manually check them and report if they are Brazilian-Portuguese or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGUTEk8QUehL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhXTB5TXR25",
        "colab_type": "text"
      },
      "source": [
        "## 1.4\n",
        "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ4TkFymXeF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
